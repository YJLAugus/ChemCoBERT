{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T08:42:45.286471Z",
     "iopub.status.busy": "2025-08-01T08:42:45.286173Z",
     "iopub.status.idle": "2025-08-01T09:05:28.416363Z",
     "shell.execute_reply": "2025-08-01T09:05:28.415603Z",
     "shell.execute_reply.started": "2025-08-01T08:42:45.286445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, roc_curve, auc,\n",
    "    confusion_matrix, cohen_kappa_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "# 关闭 RDKit 的所有日志（包括警告）\n",
    "RDLogger.DisableLog('rdApp.*')  # 禁用所有 RDKit 日志\n",
    "# 导入svg高清图库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 Matplotlib 支持中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 设置 Matplotlib 后端为 SVG\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# 设置 DPI 以提高图像清晰度\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------- Memory Protection ----------------------\n",
    "def memory_safe(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mem = psutil.virtual_memory()\n",
    "        if mem.percent > 80:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"⚠️ Memory warning: Usage {mem.percent}%, performed garbage collection\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# ---------------------- SMILES Feature Extraction ----------------------\n",
    "class SMILESFeatureExtractor:\n",
    "    def __init__(self, fp_size=1024, desc_list=None):\n",
    "        self.fp_size = fp_size\n",
    "        self.desc_list = desc_list or [\n",
    "            'MolWt', 'NumHAcceptors', 'NumHDonors', \n",
    "            'MolLogP', 'TPSA', 'NumRotatableBonds'\n",
    "        ]\n",
    "    \n",
    "    @memory_safe\n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to numerical features\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if not mol:\n",
    "                return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "            \n",
    "            # Calculate descriptors\n",
    "            desc_values = [getattr(Descriptors, desc)(mol) for desc in self.desc_list]\n",
    "            \n",
    "            # Calculate fingerprints\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=self.fp_size)\n",
    "            fp_values = np.array(fp, dtype=np.float32)\n",
    "            \n",
    "            return np.concatenate([desc_values, fp_values])\n",
    "        except:\n",
    "            return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "\n",
    "# ---------------------- Data Preparation ----------------------\n",
    "def prepare_features(X_smiles):\n",
    "    \"\"\"Convert SMILES pairs to numerical features\"\"\"\n",
    "    fe = SMILESFeatureExtractor()\n",
    "    features = []\n",
    "    \n",
    "    for drug1, drug2 in tqdm(X_smiles, desc=\"Extracting features\"):\n",
    "        feat1 = fe.smiles_to_features(drug1)\n",
    "        feat2 = fe.smiles_to_features(drug2)\n",
    "        features.append(np.concatenate([feat1, feat2]))\n",
    "    \n",
    "    X_num = np.stack(features)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X_num = np.nan_to_num(X_num)\n",
    "    return X_num\n",
    "\n",
    "# ---------------------- Deep Learning Components ----------------------\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, drug1_smiles, drug2_smiles, labels, tokenizer, max_length=128):\n",
    "        self.drug1_smiles = drug1_smiles\n",
    "        self.drug2_smiles = drug2_smiles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding1 = self.tokenizer(\n",
    "            str(self.drug1_smiles[idx]), \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer(\n",
    "            str(self.drug2_smiles[idx]),\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'drug1_input_ids': encoding1['input_ids'].flatten(),\n",
    "            'drug1_attention_mask': encoding1['attention_mask'].flatten(),\n",
    "            'drug2_input_ids': encoding2['input_ids'].flatten(),\n",
    "            'drug2_attention_mask': encoding2['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class CoAttentionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"DeepChem/ChemBERTa-77M-MLM\", hidden_size=384):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.co_attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1_input_ids, drug1_attention_mask, drug2_input_ids, drug2_attention_mask):\n",
    "        drug1 = self.bert(drug1_input_ids, attention_mask=drug1_attention_mask).last_hidden_state[:, 0, :]\n",
    "        drug2 = self.bert(drug2_input_ids, attention_mask=drug2_attention_mask).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Co-attention\n",
    "        attn1, _ = self.co_attention(drug1.unsqueeze(1), drug2.unsqueeze(1), drug2.unsqueeze(1))\n",
    "        attn2, _ = self.co_attention(drug2.unsqueeze(1), drug1.unsqueeze(1), drug1.unsqueeze(1))\n",
    "        \n",
    "        combined = torch.cat([drug1, drug2, attn1.squeeze(1), attn2.squeeze(1)], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ---------------------- Training and Evaluation ----------------------\n",
    "@memory_safe\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    best_val_auc = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': train_loss/(progress_bar.n+1)})\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"\\nValidation - Loss: {val_loss:.4f}, AUC: {val_metrics['AUC']:.4f}, F1: {val_metrics['F1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['AUC'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['AUC']\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"✅ Saved best model\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss/len(train_loader))\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_auc'].append(val_metrics['AUC'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "@memory_safe\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    y_pred = (np.array(all_probs) > 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, y_pred).ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        \"AUC\": auc_score,\n",
    "        \"Sensitivity\": tp / (tp + fn),\n",
    "        \"Specificity\": tn / (tn + fp),\n",
    "        \"Kappa\": cohen_kappa_score(all_labels, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(all_labels, y_pred),\n",
    "        \"F1\": f1_score(all_labels, y_pred),\n",
    "    }\n",
    "    \n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n",
    "# ---------------------- Main Execution ----------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    file_path = \"/kaggle/working/1/dat.txt\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    X_smiles = df.iloc[:, :2].values\n",
    "    Y = df.iloc[:, -1].values\n",
    "    \n",
    "    # Prepare numerical features for traditional models\n",
    "    X_num = prepare_features(X_smiles)\n",
    "    scaler = StandardScaler()\n",
    "    X_num = scaler.fit_transform(X_num)\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        \"ChemCoBERT\": None,\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"GBDT\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        \"K-NN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "    }\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    all_metrics = {name: [] for name in classifiers}\n",
    "    pr_curves = {name: {\"precision\": [], \"recall\": [], \"auc\": []} for name in classifiers}\n",
    "    roc_curves = {name: {\"fpr\": [], \"tpr\": [], \"auc\": []} for name in classifiers}\n",
    "    \n",
    "    # 3-fold CV\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_num)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_num, X_val_num = X_num[train_idx], X_num[val_idx]\n",
    "        X_train_smiles, X_val_smiles = X_smiles[train_idx], X_smiles[val_idx]\n",
    "        y_train, y_val = Y[train_idx], Y[val_idx]\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            if name == \"ChemCoBERT\":\n",
    "                # Deep learning model\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "                train_dataset = DrugInteractionDataset(\n",
    "                    X_train_smiles[:, 0], X_train_smiles[:, 1], y_train, tokenizer\n",
    "                )\n",
    "                val_dataset = DrugInteractionDataset(\n",
    "                    X_val_smiles[:, 0], X_val_smiles[:, 1], y_val, tokenizer\n",
    "                )\n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model = CoAttentionModel().to(device)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # Train\n",
    "                model, _ = train_model(\n",
    "                    model, \n",
    "                    DataLoader(train_dataset, batch_size=64, shuffle=True),\n",
    "                    DataLoader(val_dataset, batch_size=64),\n",
    "                    optimizer, criterion, device, epochs=10\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                _, val_metrics = evaluate_model(\n",
    "                    model, \n",
    "                    DataLoader(val_dataset, batch_size=64),\n",
    "                    criterion, device\n",
    "                )\n",
    "                \n",
    "                # Get predictions\n",
    "                model.eval()\n",
    "                all_probs = []\n",
    "                with torch.no_grad():\n",
    "                    for batch in DataLoader(val_dataset, batch_size=128):\n",
    "                        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "                        outputs = model(**inputs)\n",
    "                        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                        all_probs.extend(probs)\n",
    "                \n",
    "                y_pred_prob = np.array(all_probs)\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                \n",
    "                # Save model\n",
    "                model_path = f\"best_models/fold_{fold+1}_best.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Saved model to {model_path}\")\n",
    "                \n",
    "            else:\n",
    "                # Traditional ML models\n",
    "                clf.fit(X_train_num, y_train)\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    y_pred_prob = clf.predict_proba(X_val_num)[:, 1]\n",
    "                else:\n",
    "                    y_pred_prob = clf.decision_function(X_val_num)\n",
    "                    y_pred_prob = (y_pred_prob - y_pred_prob.min()) / (y_pred_prob.max() - y_pred_prob.min())\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                val_metrics = {\n",
    "                    \"AUC\": roc_auc_score(y_val, y_pred_prob),\n",
    "                    \"Sensitivity\": confusion_matrix(y_val, y_pred)[1,1] / (confusion_matrix(y_val, y_pred)[1,1] + confusion_matrix(y_val, y_pred)[1,0]),\n",
    "                    \"Specificity\": confusion_matrix(y_val, y_pred)[0,0] / (confusion_matrix(y_val, y_pred)[0,0] + confusion_matrix(y_val, y_pred)[0,1]),\n",
    "                    \"Kappa\": cohen_kappa_score(y_val, y_pred),\n",
    "                    \"MCC\": matthews_corrcoef(y_val, y_pred),\n",
    "                    \"F1\": f1_score(y_val, y_pred),\n",
    "                }\n",
    "            \n",
    "            # Store metrics\n",
    "            all_metrics[name].append(val_metrics)\n",
    "            \n",
    "            # Store curves\n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_curves[name][\"precision\"].append(precision)\n",
    "            pr_curves[name][\"recall\"].append(recall)\n",
    "            pr_curves[name][\"auc\"].append(pr_auc)\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_curves[name][\"fpr\"].append(fpr)\n",
    "            roc_curves[name][\"tpr\"].append(tpr)\n",
    "            roc_curves[name][\"auc\"].append(roc_auc)\n",
    "            \n",
    "            # Print fold results\n",
    "            print(f\"{name} Fold {fold+1} Results:\")\n",
    "            for metric, value in val_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for name in classifiers:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric in all_metrics[name][0].keys():\n",
    "            values = [m[metric] for m in all_metrics[name]]\n",
    "            print(f\"{metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    # Plot curves\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # PR Curve\n",
    "    plt.subplot(122)\n",
    "    for name in classifiers:\n",
    "        precision_interp = []\n",
    "        for precision, recall in zip(pr_curves[name][\"precision\"], pr_curves[name][\"recall\"]):\n",
    "            recall_interp = np.linspace(0, 1, 100)\n",
    "            precision_interp.append(np.interp(recall_interp, recall[::-1], precision[::-1]))\n",
    "        mean_precision = np.mean(precision_interp, axis=0)\n",
    "        \n",
    "        # Set ChemCoBERT to red, others to default colors\n",
    "        if name == \"ChemCoBERT\":\n",
    "            plt.plot(recall_interp, mean_precision, '#E41A1C', linewidth=3, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"GBDT\":\n",
    "            plt.plot(recall_interp, mean_precision, '#FF7F00', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Decision Tree\":\n",
    "            plt.plot(recall_interp, mean_precision, '#A65628', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Naive Bayes\":\n",
    "            plt.plot(recall_interp, mean_precision, '#377EB8', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"K-NN\":\n",
    "            plt.plot(recall_interp, mean_precision, '#984EA3', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"AdaBoost\":\n",
    "            plt.plot(recall_interp, mean_precision, '#4DAF4A', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        else:\n",
    "            plt.plot(recall_interp, mean_precision, linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "    plt.xlabel(\"Recall\", fontweight='bold')\n",
    "    plt.ylabel(\"Precision\", fontweight='bold')\n",
    "    plt.title(\"Precision Recall Curve\",fontweight='bold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Receiver Operating Characteristic Curve\n",
    "    plt.subplot(121)\n",
    "    for name in classifiers:\n",
    "        tpr_interp = []\n",
    "        for fpr, tpr in zip(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"]):\n",
    "            fpr_interp = np.linspace(0, 1, 100)\n",
    "            tpr_interp.append(np.interp(fpr_interp, fpr, tpr))\n",
    "        mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "        \n",
    "        # Set ChemCoBERT to red, others to default colors\n",
    "        if name == \"ChemCoBERT\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#E41A1C', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"GBDT\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#FF7F00', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Decision Tree\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#A65628', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Naive Bayes\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#377EB8', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"K-NN\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#984EA3', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"AdaBoost\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#4DAF4A', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        else:\n",
    "            plt.plot(fpr_interp, mean_tpr, linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "    \n",
    "   \n",
    "    plt.xlabel(\"False Positive Rate\", fontweight='bold')\n",
    "    plt.ylabel(\"True Positive Rate\", fontweight='bold')\n",
    "    plt.title(\"Receiver Operating Characteristic Curve\", fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T09:10:06.831128Z",
     "iopub.status.busy": "2025-08-01T09:10:06.830536Z",
     "iopub.status.idle": "2025-08-01T09:50:57.055461Z",
     "shell.execute_reply": "2025-08-01T09:50:57.054714Z",
     "shell.execute_reply.started": "2025-08-01T09:10:06.831103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, roc_curve, auc,\n",
    "    confusion_matrix, cohen_kappa_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "# 关闭 RDKit 的所有日志（包括警告）\n",
    "RDLogger.DisableLog('rdApp.*')  # 禁用所有 RDKit 日志\n",
    "# 导入svg高清图库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 Matplotlib 支持中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 设置 Matplotlib 后端为 SVG\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# 设置 DPI 以提高图像清晰度\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------- Memory Protection ----------------------\n",
    "def memory_safe(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mem = psutil.virtual_memory()\n",
    "        if mem.percent > 80:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"⚠️ Memory warning: Usage {mem.percent}%, performed garbage collection\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# ---------------------- SMILES Feature Extraction ----------------------\n",
    "class SMILESFeatureExtractor:\n",
    "    def __init__(self, fp_size=1024, desc_list=None):\n",
    "        self.fp_size = fp_size\n",
    "        self.desc_list = desc_list or [\n",
    "            'MolWt', 'NumHAcceptors', 'NumHDonors', \n",
    "            'MolLogP', 'TPSA', 'NumRotatableBonds'\n",
    "        ]\n",
    "    \n",
    "    @memory_safe\n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to numerical features\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if not mol:\n",
    "                return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "            \n",
    "            # Calculate descriptors\n",
    "            desc_values = [getattr(Descriptors, desc)(mol) for desc in self.desc_list]\n",
    "            \n",
    "            # Calculate fingerprints\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=self.fp_size)\n",
    "            fp_values = np.array(fp, dtype=np.float32)\n",
    "            \n",
    "            return np.concatenate([desc_values, fp_values])\n",
    "        except:\n",
    "            return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "\n",
    "# ---------------------- Data Preparation ----------------------\n",
    "def prepare_features(X_smiles):\n",
    "    \"\"\"Convert SMILES pairs to numerical features\"\"\"\n",
    "    fe = SMILESFeatureExtractor()\n",
    "    features = []\n",
    "    \n",
    "    for drug1, drug2 in tqdm(X_smiles, desc=\"Extracting features\"):\n",
    "        feat1 = fe.smiles_to_features(drug1)\n",
    "        feat2 = fe.smiles_to_features(drug2)\n",
    "        features.append(np.concatenate([feat1, feat2]))\n",
    "    \n",
    "    X_num = np.stack(features)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X_num = np.nan_to_num(X_num)\n",
    "    return X_num\n",
    "\n",
    "# ---------------------- Deep Learning Components ----------------------\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, drug1_smiles, drug2_smiles, labels, tokenizer, max_length=128):\n",
    "        self.drug1_smiles = drug1_smiles\n",
    "        self.drug2_smiles = drug2_smiles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding1 = self.tokenizer(\n",
    "            str(self.drug1_smiles[idx]), \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer(\n",
    "            str(self.drug2_smiles[idx]),\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'drug1_input_ids': encoding1['input_ids'].flatten(),\n",
    "            'drug1_attention_mask': encoding1['attention_mask'].flatten(),\n",
    "            'drug2_input_ids': encoding2['input_ids'].flatten(),\n",
    "            'drug2_attention_mask': encoding2['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class CoAttentionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"DeepChem/ChemBERTa-77M-MLM\", hidden_size=384):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.co_attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1_input_ids, drug1_attention_mask, drug2_input_ids, drug2_attention_mask):\n",
    "        drug1 = self.bert(drug1_input_ids, attention_mask=drug1_attention_mask).last_hidden_state[:, 0, :]\n",
    "        drug2 = self.bert(drug2_input_ids, attention_mask=drug2_attention_mask).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Co-attention\n",
    "        attn1, _ = self.co_attention(drug1.unsqueeze(1), drug2.unsqueeze(1), drug2.unsqueeze(1))\n",
    "        attn2, _ = self.co_attention(drug2.unsqueeze(1), drug1.unsqueeze(1), drug1.unsqueeze(1))\n",
    "        \n",
    "        combined = torch.cat([drug1, drug2, attn1.squeeze(1), attn2.squeeze(1)], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ---------------------- Training and Evaluation ----------------------\n",
    "@memory_safe\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    best_val_auc = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': train_loss/(progress_bar.n+1)})\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"\\nValidation - Loss: {val_loss:.4f}, AUC: {val_metrics['AUC']:.4f}, F1: {val_metrics['F1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['AUC'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['AUC']\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"✅ Saved best model\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss/len(train_loader))\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_auc'].append(val_metrics['AUC'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "@memory_safe\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    y_pred = (np.array(all_probs) > 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, y_pred).ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        \"AUC\": auc_score,\n",
    "        \"Sensitivity\": tp / (tp + fn),\n",
    "        \"Specificity\": tn / (tn + fp),\n",
    "        \"Kappa\": cohen_kappa_score(all_labels, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(all_labels, y_pred),\n",
    "        \"F1\": f1_score(all_labels, y_pred),\n",
    "    }\n",
    "    \n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n",
    "# ---------------------- Main Execution ----------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    file_path = \"/kaggle/working/1/dat.txt\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    X_smiles = df.iloc[:, :2].values\n",
    "    Y = df.iloc[:, -1].values\n",
    "    \n",
    "    # Prepare numerical features for traditional models\n",
    "    X_num = prepare_features(X_smiles)\n",
    "    scaler = StandardScaler()\n",
    "    X_num = scaler.fit_transform(X_num)\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        \"ChemCoBERT\": None,\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"GBDT\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        \"K-NN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "    }\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    all_metrics = {name: [] for name in classifiers}\n",
    "    pr_curves = {name: {\"precision\": [], \"recall\": [], \"auc\": []} for name in classifiers}\n",
    "    roc_curves = {name: {\"fpr\": [], \"tpr\": [], \"auc\": []} for name in classifiers}\n",
    "    \n",
    "    # 5-fold CV\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_num)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_num, X_val_num = X_num[train_idx], X_num[val_idx]\n",
    "        X_train_smiles, X_val_smiles = X_smiles[train_idx], X_smiles[val_idx]\n",
    "        y_train, y_val = Y[train_idx], Y[val_idx]\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            if name == \"ChemCoBERT\":\n",
    "                # Deep learning model\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "                train_dataset = DrugInteractionDataset(\n",
    "                    X_train_smiles[:, 0], X_train_smiles[:, 1], y_train, tokenizer\n",
    "                )\n",
    "                val_dataset = DrugInteractionDataset(\n",
    "                    X_val_smiles[:, 0], X_val_smiles[:, 1], y_val, tokenizer\n",
    "                )\n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model = CoAttentionModel().to(device)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # Train\n",
    "                model, _ = train_model(\n",
    "                    model, \n",
    "                    DataLoader(train_dataset, batch_size=64, shuffle=True),\n",
    "                    DataLoader(val_dataset, batch_size=64),\n",
    "                    optimizer, criterion, device, epochs=10\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                _, val_metrics = evaluate_model(\n",
    "                    model, \n",
    "                    DataLoader(val_dataset, batch_size=64),\n",
    "                    criterion, device\n",
    "                )\n",
    "                \n",
    "                # Get predictions\n",
    "                model.eval()\n",
    "                all_probs = []\n",
    "                with torch.no_grad():\n",
    "                    for batch in DataLoader(val_dataset, batch_size=128):\n",
    "                        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "                        outputs = model(**inputs)\n",
    "                        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                        all_probs.extend(probs)\n",
    "                \n",
    "                y_pred_prob = np.array(all_probs)\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                \n",
    "                # Save model\n",
    "                model_path = f\"best_models/fold_{fold+1}_best.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Saved model to {model_path}\")\n",
    "                \n",
    "            else:\n",
    "                # Traditional ML models\n",
    "                clf.fit(X_train_num, y_train)\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    y_pred_prob = clf.predict_proba(X_val_num)[:, 1]\n",
    "                else:\n",
    "                    y_pred_prob = clf.decision_function(X_val_num)\n",
    "                    y_pred_prob = (y_pred_prob - y_pred_prob.min()) / (y_pred_prob.max() - y_pred_prob.min())\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                val_metrics = {\n",
    "                    \"AUC\": roc_auc_score(y_val, y_pred_prob),\n",
    "                    \"Sensitivity\": confusion_matrix(y_val, y_pred)[1,1] / (confusion_matrix(y_val, y_pred)[1,1] + confusion_matrix(y_val, y_pred)[1,0]),\n",
    "                    \"Specificity\": confusion_matrix(y_val, y_pred)[0,0] / (confusion_matrix(y_val, y_pred)[0,0] + confusion_matrix(y_val, y_pred)[0,1]),\n",
    "                    \"Kappa\": cohen_kappa_score(y_val, y_pred),\n",
    "                    \"MCC\": matthews_corrcoef(y_val, y_pred),\n",
    "                    \"F1\": f1_score(y_val, y_pred),\n",
    "                }\n",
    "            \n",
    "            # Store metrics\n",
    "            all_metrics[name].append(val_metrics)\n",
    "            \n",
    "            # Store curves\n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_curves[name][\"precision\"].append(precision)\n",
    "            pr_curves[name][\"recall\"].append(recall)\n",
    "            pr_curves[name][\"auc\"].append(pr_auc)\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_curves[name][\"fpr\"].append(fpr)\n",
    "            roc_curves[name][\"tpr\"].append(tpr)\n",
    "            roc_curves[name][\"auc\"].append(roc_auc)\n",
    "            \n",
    "            # Print fold results\n",
    "            print(f\"{name} Fold {fold+1} Results:\")\n",
    "            for metric, value in val_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for name in classifiers:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric in all_metrics[name][0].keys():\n",
    "            values = [m[metric] for m in all_metrics[name]]\n",
    "            print(f\"{metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    # Plot curves\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # PR Curve\n",
    "    plt.subplot(122)\n",
    "    for name in classifiers:\n",
    "        precision_interp = []\n",
    "        for precision, recall in zip(pr_curves[name][\"precision\"], pr_curves[name][\"recall\"]):\n",
    "            recall_interp = np.linspace(0, 1, 100)\n",
    "            precision_interp.append(np.interp(recall_interp, recall[::-1], precision[::-1]))\n",
    "        mean_precision = np.mean(precision_interp, axis=0)\n",
    "        \n",
    "        # Set ChemCoBERT to red, others to default colors\n",
    "        if name == \"ChemCoBERT\":\n",
    "            plt.plot(recall_interp, mean_precision, '#E41A1C', linewidth=3, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"GBDT\":\n",
    "            plt.plot(recall_interp, mean_precision, '#FF7F00', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Decision Tree\":\n",
    "            plt.plot(recall_interp, mean_precision, '#A65628', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Naive Bayes\":\n",
    "            plt.plot(recall_interp, mean_precision, '#377EB8', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"K-NN\":\n",
    "            plt.plot(recall_interp, mean_precision, '#984EA3', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"AdaBoost\":\n",
    "            plt.plot(recall_interp, mean_precision, '#4DAF4A', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        else:\n",
    "            plt.plot(recall_interp, mean_precision, linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "    plt.xlabel(\"Recall\", fontweight='bold')\n",
    "    plt.ylabel(\"Precision\", fontweight='bold')\n",
    "    plt.title(\"Precision Recall Curve\",fontweight='bold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Receiver Operating Characteristic Curve\n",
    "    plt.subplot(121)\n",
    "    for name in classifiers:\n",
    "        tpr_interp = []\n",
    "        for fpr, tpr in zip(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"]):\n",
    "            fpr_interp = np.linspace(0, 1, 100)\n",
    "            tpr_interp.append(np.interp(fpr_interp, fpr, tpr))\n",
    "        mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "        \n",
    "        # Set ChemCoBERT to red, others to default colors\n",
    "        if name == \"ChemCoBERT\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#E41A1C', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"GBDT\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#FF7F00', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Decision Tree\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#A65628', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Naive Bayes\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#377EB8', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"K-NN\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#984EA3', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"AdaBoost\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#4DAF4A', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        else:\n",
    "            plt.plot(fpr_interp, mean_tpr, linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "    \n",
    "   \n",
    "    plt.xlabel(\"False Positive Rate\", fontweight='bold')\n",
    "    plt.ylabel(\"True Positive Rate\", fontweight='bold')\n",
    "    plt.title(\"Receiver Operating Characteristic Curve\", fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-01T23:42:40.736690Z",
     "iopub.status.busy": "2025-08-01T23:42:40.735973Z",
     "iopub.status.idle": "2025-08-02T01:08:13.307225Z",
     "shell.execute_reply": "2025-08-02T01:08:13.306464Z",
     "shell.execute_reply.started": "2025-08-01T23:42:40.736655Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, roc_curve, auc,\n",
    "    confusion_matrix, cohen_kappa_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "# 关闭 RDKit 的所有日志（包括警告）\n",
    "RDLogger.DisableLog('rdApp.*')  # 禁用所有 RDKit 日志\n",
    "# 导入svg高清图库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 Matplotlib 支持中文字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 使用黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 设置 Matplotlib 后端为 SVG\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# 设置 DPI 以提高图像清晰度\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------- Memory Protection ----------------------\n",
    "def memory_safe(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mem = psutil.virtual_memory()\n",
    "        if mem.percent > 80:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"⚠️ Memory warning: Usage {mem.percent}%, performed garbage collection\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# ---------------------- SMILES Feature Extraction ----------------------\n",
    "class SMILESFeatureExtractor:\n",
    "    def __init__(self, fp_size=1024, desc_list=None):\n",
    "        self.fp_size = fp_size\n",
    "        self.desc_list = desc_list or [\n",
    "            'MolWt', 'NumHAcceptors', 'NumHDonors', \n",
    "            'MolLogP', 'TPSA', 'NumRotatableBonds'\n",
    "        ]\n",
    "    \n",
    "    @memory_safe\n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to numerical features\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if not mol:\n",
    "                return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "            \n",
    "            # Calculate descriptors\n",
    "            desc_values = [getattr(Descriptors, desc)(mol) for desc in self.desc_list]\n",
    "            \n",
    "            # Calculate fingerprints\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=self.fp_size)\n",
    "            fp_values = np.array(fp, dtype=np.float32)\n",
    "            \n",
    "            return np.concatenate([desc_values, fp_values])\n",
    "        except:\n",
    "            return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "\n",
    "# ---------------------- Data Preparation ----------------------\n",
    "def prepare_features(X_smiles):\n",
    "    \"\"\"Convert SMILES pairs to numerical features\"\"\"\n",
    "    fe = SMILESFeatureExtractor()\n",
    "    features = []\n",
    "    \n",
    "    for drug1, drug2 in tqdm(X_smiles, desc=\"Extracting features\"):\n",
    "        feat1 = fe.smiles_to_features(drug1)\n",
    "        feat2 = fe.smiles_to_features(drug2)\n",
    "        features.append(np.concatenate([feat1, feat2]))\n",
    "    \n",
    "    X_num = np.stack(features)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X_num = np.nan_to_num(X_num)\n",
    "    return X_num\n",
    "\n",
    "# ---------------------- Deep Learning Components ----------------------\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, drug1_smiles, drug2_smiles, labels, tokenizer, max_length=128):\n",
    "        self.drug1_smiles = drug1_smiles\n",
    "        self.drug2_smiles = drug2_smiles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding1 = self.tokenizer(\n",
    "            str(self.drug1_smiles[idx]), \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer(\n",
    "            str(self.drug2_smiles[idx]),\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'drug1_input_ids': encoding1['input_ids'].flatten(),\n",
    "            'drug1_attention_mask': encoding1['attention_mask'].flatten(),\n",
    "            'drug2_input_ids': encoding2['input_ids'].flatten(),\n",
    "            'drug2_attention_mask': encoding2['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class CoAttentionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"DeepChem/ChemBERTa-77M-MLM\", hidden_size=384):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.co_attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1_input_ids, drug1_attention_mask, drug2_input_ids, drug2_attention_mask):\n",
    "        drug1 = self.bert(drug1_input_ids, attention_mask=drug1_attention_mask).last_hidden_state[:, 0, :]\n",
    "        drug2 = self.bert(drug2_input_ids, attention_mask=drug2_attention_mask).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Co-attention\n",
    "        attn1, _ = self.co_attention(drug1.unsqueeze(1), drug2.unsqueeze(1), drug2.unsqueeze(1))\n",
    "        attn2, _ = self.co_attention(drug2.unsqueeze(1), drug1.unsqueeze(1), drug1.unsqueeze(1))\n",
    "        \n",
    "        combined = torch.cat([drug1, drug2, attn1.squeeze(1), attn2.squeeze(1)], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ---------------------- Training and Evaluation ----------------------\n",
    "@memory_safe\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    best_val_auc = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': train_loss/(progress_bar.n+1)})\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"\\nValidation - Loss: {val_loss:.4f}, AUC: {val_metrics['AUC']:.4f}, F1: {val_metrics['F1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['AUC'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['AUC']\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"✅ Saved best model\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss/len(train_loader))\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_auc'].append(val_metrics['AUC'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "@memory_safe\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    y_pred = (np.array(all_probs) > 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, y_pred).ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        \"AUC\": auc_score,\n",
    "        \"Sensitivity\": tp / (tp + fn),\n",
    "        \"Specificity\": tn / (tn + fp),\n",
    "        \"Kappa\": cohen_kappa_score(all_labels, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(all_labels, y_pred),\n",
    "        \"F1\": f1_score(all_labels, y_pred),\n",
    "    }\n",
    "    \n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n",
    "# ---------------------- Main Execution ----------------------\n",
    "def main():\n",
    "    # Load data\n",
    "    file_path = \"/kaggle/working/1/dat.txt\"\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None)\n",
    "    X_smiles = df.iloc[:, :2].values\n",
    "    Y = df.iloc[:, -1].values\n",
    "    \n",
    "    # Prepare numerical features for traditional models\n",
    "    X_num = prepare_features(X_smiles)\n",
    "    scaler = StandardScaler()\n",
    "    X_num = scaler.fit_transform(X_num)\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        \"ChemCoBERT\": None,\n",
    "        \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"GBDT\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        \"K-NN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Naive Bayes\": GaussianNB(),\n",
    "    }\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    all_metrics = {name: [] for name in classifiers}\n",
    "    pr_curves = {name: {\"precision\": [], \"recall\": [], \"auc\": []} for name in classifiers}\n",
    "    roc_curves = {name: {\"fpr\": [], \"tpr\": [], \"auc\": []} for name in classifiers}\n",
    "    \n",
    "    # 10-fold CV\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_num)):\n",
    "        print(f\"\\nFold {fold + 1}\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train_num, X_val_num = X_num[train_idx], X_num[val_idx]\n",
    "        X_train_smiles, X_val_smiles = X_smiles[train_idx], X_smiles[val_idx]\n",
    "        y_train, y_val = Y[train_idx], Y[val_idx]\n",
    "        \n",
    "        for name, clf in classifiers.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            if name == \"ChemCoBERT\":\n",
    "                # Deep learning model\n",
    "                tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "                train_dataset = DrugInteractionDataset(\n",
    "                    X_train_smiles[:, 0], X_train_smiles[:, 1], y_train, tokenizer\n",
    "                )\n",
    "                val_dataset = DrugInteractionDataset(\n",
    "                    X_val_smiles[:, 0], X_val_smiles[:, 1], y_val, tokenizer\n",
    "                )\n",
    "                \n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                model = CoAttentionModel().to(device)\n",
    "                optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                \n",
    "                # Train\n",
    "                model, _ = train_model(\n",
    "                    model, \n",
    "                    DataLoader(train_dataset, batch_size=64, shuffle=True),\n",
    "                    DataLoader(val_dataset, batch_size=64),\n",
    "                    optimizer, criterion, device, epochs=10\n",
    "                )\n",
    "                \n",
    "                # Evaluate\n",
    "                model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                _, val_metrics = evaluate_model(\n",
    "                    model, \n",
    "                    DataLoader(val_dataset, batch_size=64),\n",
    "                    criterion, device\n",
    "                )\n",
    "                \n",
    "                # Get predictions\n",
    "                model.eval()\n",
    "                all_probs = []\n",
    "                with torch.no_grad():\n",
    "                    for batch in DataLoader(val_dataset, batch_size=128):\n",
    "                        inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "                        outputs = model(**inputs)\n",
    "                        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "                        all_probs.extend(probs)\n",
    "                \n",
    "                y_pred_prob = np.array(all_probs)\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                \n",
    "                # Save model\n",
    "                model_path = f\"best_models/fold_{fold+1}_best.pth\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                print(f\"Saved model to {model_path}\")\n",
    "                \n",
    "            else:\n",
    "                # Traditional ML models\n",
    "                clf.fit(X_train_num, y_train)\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    y_pred_prob = clf.predict_proba(X_val_num)[:, 1]\n",
    "                else:\n",
    "                    y_pred_prob = clf.decision_function(X_val_num)\n",
    "                    y_pred_prob = (y_pred_prob - y_pred_prob.min()) / (y_pred_prob.max() - y_pred_prob.min())\n",
    "                y_pred = (y_pred_prob > 0.5).astype(int)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                val_metrics = {\n",
    "                    \"AUC\": roc_auc_score(y_val, y_pred_prob),\n",
    "                    \"Sensitivity\": confusion_matrix(y_val, y_pred)[1,1] / (confusion_matrix(y_val, y_pred)[1,1] + confusion_matrix(y_val, y_pred)[1,0]),\n",
    "                    \"Specificity\": confusion_matrix(y_val, y_pred)[0,0] / (confusion_matrix(y_val, y_pred)[0,0] + confusion_matrix(y_val, y_pred)[0,1]),\n",
    "                    \"Kappa\": cohen_kappa_score(y_val, y_pred),\n",
    "                    \"MCC\": matthews_corrcoef(y_val, y_pred),\n",
    "                    \"F1\": f1_score(y_val, y_pred),\n",
    "                }\n",
    "            \n",
    "            # Store metrics\n",
    "            all_metrics[name].append(val_metrics)\n",
    "            \n",
    "            # Store curves\n",
    "            precision, recall, _ = precision_recall_curve(y_val, y_pred_prob)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_curves[name][\"precision\"].append(precision)\n",
    "            pr_curves[name][\"recall\"].append(recall)\n",
    "            pr_curves[name][\"auc\"].append(pr_auc)\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_val, y_pred_prob)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_curves[name][\"fpr\"].append(fpr)\n",
    "            roc_curves[name][\"tpr\"].append(tpr)\n",
    "            roc_curves[name][\"auc\"].append(roc_auc)\n",
    "            \n",
    "            # Print fold results\n",
    "            print(f\"{name} Fold {fold+1} Results:\")\n",
    "            for metric, value in val_metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Print final metrics\n",
    "    print(\"\\nFinal Metrics:\")\n",
    "    for name in classifiers:\n",
    "        print(f\"\\n{name}:\")\n",
    "        for metric in all_metrics[name][0].keys():\n",
    "            values = [m[metric] for m in all_metrics[name]]\n",
    "            print(f\"{metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")\n",
    "    \n",
    "    # Plot curves\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # PR Curve\n",
    "    plt.subplot(122)\n",
    "    for name in classifiers:\n",
    "        precision_interp = []\n",
    "        for precision, recall in zip(pr_curves[name][\"precision\"], pr_curves[name][\"recall\"]):\n",
    "            recall_interp = np.linspace(0, 1, 100)\n",
    "            precision_interp.append(np.interp(recall_interp, recall[::-1], precision[::-1]))\n",
    "        mean_precision = np.mean(precision_interp, axis=0)\n",
    "        \n",
    "        # Set ChemCoBERT to red, others to default colors\n",
    "        if name == \"ChemCoBERT\":\n",
    "            plt.plot(recall_interp, mean_precision, '#E41A1C', linewidth=3, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"GBDT\":\n",
    "            plt.plot(recall_interp, mean_precision, '#FF7F00', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Decision Tree\":\n",
    "            plt.plot(recall_interp, mean_precision, '#A65628', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Naive Bayes\":\n",
    "            plt.plot(recall_interp, mean_precision, '#377EB8', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"K-NN\":\n",
    "            plt.plot(recall_interp, mean_precision, '#984EA3', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"AdaBoost\":\n",
    "            plt.plot(recall_interp, mean_precision, '#4DAF4A', linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "        else:\n",
    "            plt.plot(recall_interp, mean_precision, linewidth=2.5, label=f\"{name} (AUC={np.mean(pr_curves[name]['auc']):.4f})\")\n",
    "    plt.xlabel(\"Recall\", fontweight='bold')\n",
    "    plt.ylabel(\"Precision\", fontweight='bold')\n",
    "    plt.title(\"Precision Recall Curve\",fontweight='bold')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Receiver Operating Characteristic Curve\n",
    "    plt.subplot(121)\n",
    "    for name in classifiers:\n",
    "        tpr_interp = []\n",
    "        for fpr, tpr in zip(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"]):\n",
    "            fpr_interp = np.linspace(0, 1, 100)\n",
    "            tpr_interp.append(np.interp(fpr_interp, fpr, tpr))\n",
    "        mean_tpr = np.mean(tpr_interp, axis=0)\n",
    "        \n",
    "        # Set ChemCoBERT to red, others to default colors\n",
    "        if name == \"ChemCoBERT\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#E41A1C', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"GBDT\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#FF7F00', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Decision Tree\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#A65628', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"Naive Bayes\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#377EB8', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"K-NN\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#984EA3', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        elif name == \"AdaBoost\":\n",
    "            plt.plot(fpr_interp, mean_tpr, '#4DAF4A', linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "        else:\n",
    "            plt.plot(fpr_interp, mean_tpr, linewidth=2.5, \n",
    "                    label=f\"{name} (AUC={np.mean(roc_curves[name]['auc']):.4f})\")\n",
    "    \n",
    "   \n",
    "    plt.xlabel(\"False Positive Rate\", fontweight='bold')\n",
    "    plt.ylabel(\"True Positive Rate\", fontweight='bold')\n",
    "    plt.title(\"Receiver Operating Characteristic Curve\", fontweight='bold')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7984808,
     "sourceId": 12636268,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
