{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-21T12:36:57.318310Z",
     "iopub.status.busy": "2025-08-21T12:36:57.317984Z",
     "iopub.status.idle": "2025-08-21T12:45:04.711662Z",
     "shell.execute_reply": "2025-08-21T12:45:04.711012Z",
     "shell.execute_reply.started": "2025-08-21T12:36:57.318285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, roc_curve, auc,\n",
    "    confusion_matrix, cohen_kappa_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "import joblib\n",
    "\n",
    "# 环境设置\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# ---------------------- 内存安全装饰器 ----------------------\n",
    "def memory_safe(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mem = psutil.virtual_memory()\n",
    "        if mem.percent > 80:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"⚠️ Memory warning: Usage {mem.percent}%, performed garbage collection\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# ---------------------- SMILES特征提取 ----------------------\n",
    "class SMILESFeatureExtractor:\n",
    "    def __init__(self, fp_size=1024, desc_list=None):\n",
    "        self.fp_size = fp_size\n",
    "        self.desc_list = desc_list or [\n",
    "            'MolWt', 'NumHAcceptors', 'NumHDonors', \n",
    "            'MolLogP', 'TPSA', 'NumRotatableBonds'\n",
    "        ]\n",
    "    \n",
    "    @memory_safe\n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to numerical features\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if not mol:\n",
    "                return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "            \n",
    "            # 计算描述符\n",
    "            desc_values = [getattr(Descriptors, desc)(mol) for desc in self.desc_list]\n",
    "            \n",
    "            # 计算指纹\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=self.fp_size)\n",
    "            fp_values = np.array(fp, dtype=np.float32)\n",
    "            \n",
    "            return np.concatenate([desc_values, fp_values])\n",
    "        except:\n",
    "            return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "\n",
    "# ---------------------- 数据准备 ----------------------\n",
    "def prepare_features(X_smiles):\n",
    "    \"\"\"Convert SMILES pairs to numerical features\"\"\"\n",
    "    fe = SMILESFeatureExtractor()\n",
    "    features = []\n",
    "    \n",
    "    for drug1, drug2 in tqdm(X_smiles, desc=\"Extracting features\"):\n",
    "        feat1 = fe.smiles_to_features(drug1)\n",
    "        feat2 = fe.smiles_to_features(drug2)\n",
    "        features.append(np.concatenate([feat1, feat2]))\n",
    "    \n",
    "    X_num = np.stack(features)\n",
    "    return np.nan_to_num(X_num)\n",
    "\n",
    "# ---------------------- 深度学习组件 ----------------------\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, drug1_smiles, drug2_smiles, labels=None, tokenizer=None, max_length=128):\n",
    "        self.drug1_smiles = drug1_smiles\n",
    "        self.drug2_smiles = drug2_smiles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.drug1_smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding1 = self.tokenizer(\n",
    "            str(self.drug1_smiles[idx]), \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer(\n",
    "            str(self.drug2_smiles[idx]),\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'drug1_input_ids': encoding1['input_ids'].flatten(),\n",
    "            'drug1_attention_mask': encoding1['attention_mask'].flatten(),\n",
    "            'drug2_input_ids': encoding2['input_ids'].flatten(),\n",
    "            'drug2_attention_mask': encoding2['attention_mask'].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "class CoAttentionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"DeepChem/ChemBERTa-77M-MLM\", hidden_size=384):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.co_attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1_input_ids, drug1_attention_mask, drug2_input_ids, drug2_attention_mask):\n",
    "        drug1 = self.bert(drug1_input_ids, attention_mask=drug1_attention_mask).last_hidden_state[:, 0, :]\n",
    "        drug2 = self.bert(drug2_input_ids, attention_mask=drug2_attention_mask).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # 协同注意力机制\n",
    "        attn1, _ = self.co_attention(drug1.unsqueeze(1), drug2.unsqueeze(1), drug2.unsqueeze(1))\n",
    "        attn2, _ = self.co_attention(drug2.unsqueeze(1), drug1.unsqueeze(1), drug1.unsqueeze(1))\n",
    "        \n",
    "        combined = torch.cat([drug1, drug2, attn1.squeeze(1), attn2.squeeze(1)], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ---------------------- 评估相关函数 ----------------------\n",
    "def find_optimal_cutoff(tpr, fpr, thresholds):\n",
    "    \"\"\"使用Youden's J统计量寻找最佳阈值\"\"\"\n",
    "    youden = tpr - fpr\n",
    "    return thresholds[np.argmax(youden)]\n",
    "\n",
    "def best_confusion_matrix(y_true, y_prob, y_train=None, train_prob=None):\n",
    "    \"\"\"\n",
    "    计算最佳阈值和混淆矩阵\n",
    "    优先级: 测试集ROC > 训练集ROC > 默认0.5\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 优先使用测试集数据\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_prob, pos_label=1)\n",
    "        cutoff = find_optimal_cutoff(tpr, fpr, thresholds)\n",
    "        y_pred = (y_prob >= cutoff).astype(int)\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        return cutoff, tn, fp, fn, tp\n",
    "    except ValueError:\n",
    "        # 测试集全0/全1时，尝试使用训练集阈值\n",
    "        if y_train is not None and train_prob is not None:\n",
    "            try:\n",
    "                fpr, tpr, thresholds = roc_curve(y_train, train_prob, pos_label=1)\n",
    "                cutoff = find_optimal_cutoff(tpr, fpr, thresholds)\n",
    "                print(f\"⚠️ 使用训练集确定的最佳阈值: {cutoff:.4f}\")\n",
    "                y_pred = (y_prob >= cutoff).astype(int)\n",
    "                if np.all(y_true == 1):  # 处理全1标签的情况\n",
    "                    tp = np.sum(y_pred == 1)\n",
    "                    fn = len(y_true) - tp\n",
    "                    return cutoff, 0, 0, fn, tp\n",
    "                elif np.all(y_true == 0):  # 处理全0标签的情况\n",
    "                    tn = np.sum(y_pred == 0)\n",
    "                    fp = len(y_true) - tn\n",
    "                    return cutoff, tn, fp, 0, 0\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 最终回退到默认阈值\n",
    "        print(\"⚠️ 使用默认阈值0.5\")\n",
    "        cutoff = 0.5\n",
    "        y_pred = (y_prob >= cutoff).astype(int)\n",
    "        if np.all(y_true == 1):  # 处理全1标签的情况\n",
    "            tp = np.sum(y_pred == 1)\n",
    "            fn = len(y_true) - tp\n",
    "            return cutoff, 0, 0, fn, tp\n",
    "        elif np.all(y_true == 0):  # 处理全0标签的情况\n",
    "            tn = np.sum(y_pred == 0)\n",
    "            fp = len(y_true) - tn\n",
    "            return cutoff, tn, fp, 0, 0\n",
    "        else:\n",
    "            return cutoff, 0, 0, 0, 0  # 其他未知情况\n",
    "\n",
    "@memory_safe\n",
    "@memory_safe\n",
    "def evaluate_model(model, data_loader, criterion, device, y_train=None, train_prob=None):\n",
    "    \"\"\"综合评估模型性能\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # 处理极端情况\n",
    "    unique_labels = np.unique(all_labels)\n",
    "    if len(unique_labels) == 1:\n",
    "        print(f\"⚠️ 测试集标签全为{unique_labels[0]}，启用后备指标计算\")\n",
    "        # 对于单一类别，设置默认指标值\n",
    "        if unique_labels[0] == 1:  # 全1标签\n",
    "            metrics = {\n",
    "                \"AUC\": 1.0,\n",
    "                \"Cutoff\": 0.5,\n",
    "                \"Sensitivity\": 1.0,\n",
    "                \"Specificity\": 0.0,\n",
    "                \"Precision\": 1.0,\n",
    "                \"F1\": 1.0,\n",
    "                \"MCC\": 0.0,\n",
    "                \"Kappa\": 0.0,\n",
    "            }\n",
    "        else:  # 全0标签\n",
    "            metrics = {\n",
    "                \"AUC\": 1.0,\n",
    "                \"Cutoff\": 0.5,\n",
    "                \"Sensitivity\": 0.0,\n",
    "                \"Specificity\": 1.0,\n",
    "                \"Precision\": 0.0,\n",
    "                \"F1\": 0.0,\n",
    "                \"MCC\": 0.0,\n",
    "                \"Kappa\": 0.0,\n",
    "            }\n",
    "        return total_loss / len(data_loader), metrics\n",
    "    \n",
    "    # 正常情况下的指标计算\n",
    "    cutoff, tn, fp, fn, tp = best_confusion_matrix(all_labels, all_probs, y_train, train_prob)\n",
    "    \n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(all_labels, all_probs),\n",
    "        \"Cutoff\": cutoff,\n",
    "        \"Sensitivity\": tp / (tp + fn) if (tp + fn) > 0 else 0.0,\n",
    "        \"Specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
    "        \"Precision\": tp / (tp + fp) if (tp + fp) > 0 else 0.0,\n",
    "        \"F1\": f1_score(all_labels, (all_probs >= cutoff).astype(int)),\n",
    "        \"MCC\": matthews_corrcoef(all_labels, (all_probs >= cutoff).astype(int)),\n",
    "        \"Kappa\": cohen_kappa_score(all_labels, (all_probs >= cutoff).astype(int)),\n",
    "    }\n",
    "    \n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n",
    "\n",
    "# ---------------------- 训练函数 ----------------------\n",
    "@memory_safe\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    \"\"\"训练深度学习模型\"\"\"\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_train_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_train_loss += loss.item()\n",
    "        \n",
    "        # 计算验证损失\n",
    "        val_loss, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        # 记录损失\n",
    "        train_losses.append(epoch_train_loss / len(train_loader))\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} - Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "    \n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(best_model)\n",
    "    return model, (train_losses, val_losses)\n",
    "\n",
    "# ---------------------- 预测函数 ----------------------\n",
    "@memory_safe\n",
    "def predict(model, data_loader, device):\n",
    "    \"\"\"使用模型进行预测\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "            all_probs.extend(probs)\n",
    "            \n",
    "            if 'label' in batch:\n",
    "                all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    return np.array(all_probs), np.array(all_labels) if len(all_labels) > 0 else None\n",
    "\n",
    "# ---------------------- 保存预测结果函数 ----------------------\n",
    "def save_prediction_scores(model_name, smiles_pairs, probs, labels, cutoff):\n",
    "    \"\"\"保存模型预测结果\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Drug1_SMILES': smiles_pairs[:, 0],\n",
    "        'Drug2_SMILES': smiles_pairs[:, 1],\n",
    "        'Predicted_Probability': probs,\n",
    "        'Predicted_Label': (probs >= cutoff).astype(int)\n",
    "    })\n",
    "    \n",
    "    if labels is not None:\n",
    "        df['True_Label'] = labels\n",
    "    \n",
    "    df.to_csv(f\"{model_name.replace(' ', '_')}_predictions.csv\", index=False)\n",
    "    print(f\"✅ {model_name} 预测结果已保存\")\n",
    "\n",
    "# ---------------------- 主执行流程 ----------------------\n",
    "def main():\n",
    "    # 数据加载\n",
    "    train_df = pd.read_csv(\"/kaggle/working/3/3-version2/train.txt\", sep='\\t', header=None)\n",
    "    X_train_smiles = train_df.iloc[:, :2].values\n",
    "    y_train = train_df.iloc[:, -1].values\n",
    "    \n",
    "    test_df = pd.read_csv(\"/kaggle/working/3/3-version2/test.txt\", sep='\\t', header=None)\n",
    "    X_test_smiles = test_df.iloc[:, :2].values\n",
    "    y_test = test_df.iloc[:, -1].values if test_df.shape[1] > 2 else None\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # 检查数据平衡性\n",
    "    print(\"训练集类别分布:\", np.unique(y_train, return_counts=True))\n",
    "    if y_test is not None:\n",
    "        print(\"测试集类别分布:\", np.unique(y_test, return_counts=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##########\n",
    "    \n",
    "    # 特征工程\n",
    "    print(\"\\nPreparing features...\")\n",
    "    X_train_num = prepare_features(X_train_smiles)\n",
    "    X_test_num = prepare_features(X_test_smiles)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "    # 模型定义\n",
    "    classifiers = {\n",
    "        \"ChemCoBERT\": None,\n",
    "        \"Decision Tree\": DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"GBDT\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        \"K-NN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Naive Bayes\": GaussianNB(var_smoothing=1e-2)\n",
    "    }\n",
    "    \n",
    "    # 训练和评估\n",
    "\n",
    "    # 训练和评估\n",
    "    all_metrics = {}\n",
    "    pr_curves = {}\n",
    "    roc_curves = {}\n",
    "    \n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n{'='*50}\\nTraining {name}...\\n{'='*50}\")\n",
    "        \n",
    "        if name == \"ChemCoBERT\":\n",
    "            # 深度学习模型流程\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "            train_dataset = DrugInteractionDataset(X_train_smiles[:, 0], X_train_smiles[:, 1], y_train, tokenizer)\n",
    "            test_dataset = DrugInteractionDataset(X_test_smiles[:, 0], X_test_smiles[:, 1], y_test, tokenizer)\n",
    "            \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = CoAttentionModel().to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # 训练\n",
    "            model, _ = train_model(\n",
    "                model, \n",
    "                DataLoader(train_dataset, batch_size=8, shuffle=True),\n",
    "                DataLoader(test_dataset, batch_size=8),\n",
    "                optimizer, criterion, device, epochs=10\n",
    "            )\n",
    "            \n",
    "            # 评估\n",
    "            if y_test is not None:\n",
    "                # 获取训练集预测概率（用于后备阈值）\n",
    "                train_probs, _ = predict(model, DataLoader(train_dataset, batch_size=128), device)\n",
    "                \n",
    "                # 评估测试集\n",
    "                _, test_metrics = evaluate_model(\n",
    "                    model, \n",
    "                    DataLoader(test_dataset, batch_size=8),\n",
    "                    criterion, \n",
    "                    device,\n",
    "                    y_train,\n",
    "                    train_probs\n",
    "                )\n",
    "                all_metrics[name] = test_metrics\n",
    "                \n",
    "                # 保存预测结果\n",
    "                test_probs, _ = predict(model, DataLoader(test_dataset, batch_size=128), device)\n",
    "                save_prediction_scores(name, X_test_smiles, test_probs, y_test, test_metrics['Cutoff'])\n",
    "            \n",
    "            # 保存模型\n",
    "            torch.save(model.state_dict(), f\"{name.replace(' ', '_')}_model.pth\")\n",
    "\n",
    "        else:\n",
    "            # 传统机器学习流程\n",
    "            clf.fit(X_train_num, y_train)\n",
    "            joblib.dump(clf, f\"{name.replace(' ', '_')}_model.joblib\")\n",
    "            \n",
    "            if y_test is not None:\n",
    "                # 获取训练集预测概率\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    train_probs = clf.predict_proba(X_train_num)[:, 1]\n",
    "                else:\n",
    "                    train_probs = clf.decision_function(X_train_num)\n",
    "                    train_probs = (train_probs - train_probs.min()) / (train_probs.max() - train_probs.min())\n",
    "                \n",
    "                # 测试集预测\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    test_probs = clf.predict_proba(X_test_num)[:, 1]\n",
    "                else:\n",
    "                    test_probs = clf.decision_function(X_test_num)\n",
    "                    test_probs = (test_probs - test_probs.min()) / (test_probs.max() - test_probs.min())\n",
    "                \n",
    "                # 计算指标\n",
    "                cutoff, tn, fp, fn, tp = best_confusion_matrix(\n",
    "                    y_test, test_probs, y_train, train_probs\n",
    "                )\n",
    "                \n",
    "                all_metrics[name] = {\n",
    "                    \"AUC\": roc_auc_score(y_test, test_probs) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "                    \"Cutoff\": cutoff,\n",
    "                    \"Sensitivity\": tp / (tp + fn) if (tp + fn) > 0 else 1.0 * (np.mean(y_test) == 1),\n",
    "                    \"Specificity\": tn / (tn + fp) if (tn + fp) > 0 else 1.0 * (np.mean(y_test) == 0),\n",
    "                    \"Precision\": tp / (tp + fp) if (tp + fp) > 0 else 0.0,\n",
    "                    \"F1\": f1_score(y_test, (test_probs >= cutoff).astype(int)),\n",
    "                    \"MCC\": matthews_corrcoef(y_test, (test_probs >= cutoff).astype(int)),\n",
    "                    \"Kappa\": cohen_kappa_score(y_test, (test_probs >= cutoff).astype(int)),\n",
    "                }\n",
    "                \n",
    "                # 保存预测结果\n",
    "                save_prediction_scores(name, X_test_smiles, test_probs, y_test, cutoff)\n",
    "    \n",
    "    # 输出所有模型的评估结果\n",
    "    print(\"\\n\\n=== 模型性能比较 ===\")\n",
    "    metrics_df = pd.DataFrame(all_metrics).T\n",
    "    print(metrics_df)\n",
    "    \n",
    "    # 保存评估结果\n",
    "    metrics_df.to_csv(\"model_performance_comparison.csv\")\n",
    "    print(\"\\n✅ 所有模型评估结果已保存到 model_performance_comparison.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8110685,
     "sourceId": 12825345,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
