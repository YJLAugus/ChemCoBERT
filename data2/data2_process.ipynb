{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-19T23:54:36.503778Z",
     "iopub.status.busy": "2025-08-19T23:54:36.503499Z",
     "iopub.status.idle": "2025-08-19T23:56:35.515990Z",
     "shell.execute_reply": "2025-08-19T23:56:35.515236Z",
     "shell.execute_reply.started": "2025-08-19T23:54:36.503752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_recall_curve, roc_curve, auc,\n",
    "    confusion_matrix, cohen_kappa_score, matthews_corrcoef, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, AllChem\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import psutil\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, auc, confusion_matrix, cohen_kappa_score, matthews_corrcoef, f1_score\n",
    "\n",
    "\n",
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "# 关闭 RDKit 的所有日志（包括警告）\n",
    "RDLogger.DisableLog('rdApp.*')  # 禁用所有 RDKit 日志\n",
    "# 导入svg高清图库\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置 Matplotlib 支持中文字体\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
    "\n",
    "# 设置 Matplotlib 后端为 SVG\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "# 设置 DPI 以提高图像清晰度\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------------- Memory Protection ----------------------\n",
    "def memory_safe(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        mem = psutil.virtual_memory()\n",
    "        if mem.percent > 80:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(f\"⚠️ Memory warning: Usage {mem.percent}%, performed garbage collection\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n",
    "\n",
    "# ---------------------- SMILES Feature Extraction ----------------------\n",
    "class SMILESFeatureExtractor:\n",
    "    def __init__(self, fp_size=1024, desc_list=None):\n",
    "        self.fp_size = fp_size\n",
    "        self.desc_list = desc_list or [\n",
    "           'MolWt', 'NumHAcceptors', 'NumHDonors', \n",
    "           'MolLogP', 'TPSA', 'NumRotatableBonds'\n",
    "          \n",
    "        ]\n",
    "    \n",
    "    @memory_safe\n",
    "    def smiles_to_features(self, smiles):\n",
    "        \"\"\"Convert SMILES to numerical features\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if not mol:\n",
    "                return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "            \n",
    "            # Calculate descriptors\n",
    "            desc_values = [getattr(Descriptors, desc)(mol) for desc in self.desc_list]\n",
    "            \n",
    "            # Calculate fingerprints\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=2, nBits=self.fp_size)\n",
    "            fp_values = np.array(fp, dtype=np.float32)\n",
    "            \n",
    "            return np.concatenate([desc_values, fp_values])\n",
    "        except:\n",
    "            return np.nan * np.ones(len(self.desc_list) + self.fp_size)\n",
    "\n",
    "# ---------------------- Data Preparation ----------------------\n",
    "def prepare_features(X_smiles):\n",
    "    \"\"\"Convert SMILES pairs to numerical features\"\"\"\n",
    "    fe = SMILESFeatureExtractor()\n",
    "    features = []\n",
    "    \n",
    "    for drug1, drug2 in tqdm(X_smiles, desc=\"Extracting features\"):\n",
    "        feat1 = fe.smiles_to_features(drug1)\n",
    "        feat2 = fe.smiles_to_features(drug2)\n",
    "        features.append(np.concatenate([feat1, feat2]))\n",
    "    \n",
    "    X_num = np.stack(features)\n",
    "    \n",
    "    # Handle NaN values\n",
    "    X_num = np.nan_to_num(X_num)\n",
    "    return X_num\n",
    "\n",
    "# ---------------------- Deep Learning Components ----------------------\n",
    "class DrugInteractionDataset(Dataset):\n",
    "    def __init__(self, drug1_smiles, drug2_smiles, labels=None, tokenizer=None, max_length=128):\n",
    "        self.drug1_smiles = drug1_smiles\n",
    "        self.drug2_smiles = drug2_smiles\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.drug1_smiles)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoding1 = self.tokenizer(\n",
    "            str(self.drug1_smiles[idx]), \n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding2 = self.tokenizer(\n",
    "            str(self.drug2_smiles[idx]),\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        item = {\n",
    "            'drug1_input_ids': encoding1['input_ids'].flatten(),\n",
    "            'drug1_attention_mask': encoding1['attention_mask'].flatten(),\n",
    "            'drug2_input_ids': encoding2['input_ids'].flatten(),\n",
    "            'drug2_attention_mask': encoding2['attention_mask'].flatten(),\n",
    "        }\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            \n",
    "        return item\n",
    "\n",
    "\n",
    "class CoAttentionModel(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"DeepChem/ChemBERTa-77M-MLM\", hidden_size=384):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(bert_model_name)\n",
    "        self.co_attention = nn.MultiheadAttention(hidden_size, num_heads=8)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size*4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, drug1_input_ids, drug1_attention_mask, drug2_input_ids, drug2_attention_mask):\n",
    "        drug1 = self.bert(drug1_input_ids, attention_mask=drug1_attention_mask).last_hidden_state[:, 0, :]\n",
    "        drug2 = self.bert(drug2_input_ids, attention_mask=drug2_attention_mask).last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Co-attention\n",
    "        attn1, _ = self.co_attention(drug1.unsqueeze(1), drug2.unsqueeze(1), drug2.unsqueeze(1))\n",
    "        attn2, _ = self.co_attention(drug2.unsqueeze(1), drug1.unsqueeze(1), drug1.unsqueeze(1))\n",
    "        \n",
    "        combined = torch.cat([drug1, drug2, attn1.squeeze(1), attn2.squeeze(1)], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "# ---------------------- Training and Evaluation ----------------------\n",
    "@memory_safe\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, device, epochs=10):\n",
    "    best_val_auc = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': train_loss/(progress_bar.n+1)})\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_metrics = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f\"\\nValidation - Loss: {val_loss:.4f}, AUC: {val_metrics['AUC']:.4f}, F1: {val_metrics['F1']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['AUC'] > best_val_auc:\n",
    "            best_val_auc = val_metrics['AUC']\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"✅ Saved best model\")\n",
    "        \n",
    "        history['train_loss'].append(train_loss/len(train_loader))\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_auc'].append(val_metrics['AUC'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def find_optimal_cutoff(tpr, fpr, thresholds):\n",
    "    \"\"\"Find optimal cutoff point using Youden's J statistic\"\"\"\n",
    "    youden = tpr - fpr\n",
    "    return thresholds[np.argmax(youden)]\n",
    "\n",
    "def best_confusion_matrix(y_test, y_test_predprob):\n",
    "    \"\"\"Calculate metrics using optimal cutoff\"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predprob, pos_label=1)\n",
    "    cutoff = find_optimal_cutoff(tpr, fpr, thresholds)\n",
    "    y_pred = list(map(lambda x: 1 if x >= cutoff else 0, y_test_predprob))\n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    return cutoff, TN, FN, FP, TP\n",
    "\n",
    "@memory_safe\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'label'}\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs, batch['label'].to(device))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_labels.extend(batch['label'].cpu().numpy())\n",
    "            all_probs.extend(probs)\n",
    "    \n",
    "    # 使用最佳阈值计算指标\n",
    "    cutoff, TN, FP, FN, TP = best_confusion_matrix(all_labels, all_probs)\n",
    "    \n",
    "    metrics = {\n",
    "        \"AUC\": roc_auc_score(all_labels, all_probs),\n",
    "        \"Cutoff\": cutoff,\n",
    "        \"Sensitivity\": TP / (TP + FN),\n",
    "        \"Specificity\": TN / (TN + FP),\n",
    "        \"Kappa\": cohen_kappa_score(all_labels, (np.array(all_probs) >= cutoff).astype(int)),\n",
    "        \"MCC\": matthews_corrcoef(all_labels, (np.array(all_probs) >= cutoff).astype(int)),\n",
    "        \"F1\": f1_score(all_labels, (np.array(all_probs) >= cutoff).astype(int)),\n",
    "    }\n",
    "    \n",
    "    return total_loss / len(data_loader), metrics\n",
    "\n",
    "@memory_safe\n",
    "def predict(model, data_loader, device):\n",
    "    \"\"\"Make predictions on new data\"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Predicting\"):\n",
    "            # 只传递模型需要的输入参数，排除label\n",
    "            inputs = {\n",
    "                'drug1_input_ids': batch['drug1_input_ids'].to(device),\n",
    "                'drug1_attention_mask': batch['drug1_attention_mask'].to(device),\n",
    "                'drug2_input_ids': batch['drug2_input_ids'].to(device),\n",
    "                'drug2_attention_mask': batch['drug2_attention_mask'].to(device)\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_probs.extend(probs)\n",
    "            \n",
    "            # 如果有标签，也收集标签\n",
    "            if 'label' in batch:\n",
    "                all_labels.extend(batch['label'].cpu().numpy())\n",
    "    \n",
    "    return np.array(all_probs), np.array(all_labels) if all_labels else None\n",
    "\n",
    "def save_prediction_scores(model_name, smiles_pairs, probs, labels=None, cutoff=None):\n",
    "    \"\"\"Save prediction scores with original labels to CSV\"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'Drug1_SMILES': smiles_pairs[:, 0],\n",
    "        'Drug2_SMILES': smiles_pairs[:, 1],\n",
    "        'Prediction_Score': probs\n",
    "    })\n",
    "    \n",
    "    if labels is not None:\n",
    "        df['Original_Label'] = labels\n",
    "    \n",
    "    if cutoff is not None:\n",
    "        df['Predicted_Label'] = (probs >= cutoff).astype(int)\n",
    "    \n",
    "    filename = f\"{model_name.replace(' ', '_')}_prediction_scores.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"✅ {model_name} prediction scores saved to {filename}\")\n",
    "\n",
    "# ---------------------- Main Execution ----------------------\n",
    "def main():\n",
    "    # Load training data\n",
    "    train_df = pd.read_csv(\"/kaggle/working/222222/222222/2/train.txt\", sep='\\t', header=None)\n",
    "    X_train_smiles = train_df.iloc[:, :2].values\n",
    "    y_train = train_df.iloc[:, -1].values\n",
    "    \n",
    "    # Load test data\n",
    "    test_df = pd.read_csv(\"/kaggle/working/222222/222222/2/test.txt\", sep='\\t', header=None)\n",
    "    X_test_smiles = test_df.iloc[:, :2].values\n",
    "    y_test = test_df.iloc[:, -1].values if test_df.shape[1] > 2 else None\n",
    "    \n",
    "    # Prepare numerical features for traditional models\n",
    "    print(\"\\nPreparing features...\")\n",
    "    X_train_num = prepare_features(X_train_smiles)\n",
    "    X_test_num = prepare_features(X_test_smiles)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_num = scaler.fit_transform(X_train_num)\n",
    "    X_test_num = scaler.transform(X_test_num)\n",
    "\n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        \"ChemCoBERT\": None,\n",
    "        \"Decision Tree\": DecisionTreeClassifier(\n",
    "            max_depth=5,                # 大幅降低深度\n",
    "            min_samples_split=20,       # 增大分裂门槛\n",
    "            min_samples_leaf=10,        # 增大叶节点样本数\n",
    "            max_features=0.8,           # 改用比例而非\"sqrt\"\n",
    "            criterion=\"gini\",           # 换回gini尝试\n",
    "            class_weight=\"balanced\",\n",
    "            random_state=42\n",
    "        ),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=100, random_state=42),\n",
    "        \"GBDT\": GradientBoostingClassifier(\n",
    "            n_estimators=100, \n",
    "            random_state=42,  # 统一随机种子\n",
    "            learning_rate=0.1,  # 不同的学习率\n",
    "            max_depth=2,        # 不同的最大深度\n",
    "            subsample=0.5,      # 添加子采样\n",
    "            max_features='sqrt' # 添加特征子采样\n",
    "        ),\n",
    "        \"K-NN\": KNeighborsClassifier(n_neighbors=5),\n",
    "        \"Naive Bayes\": GaussianNB(var_smoothing=1e-2)\n",
    "    }\n",
    "    \n",
    "    # Initialize metrics storage\n",
    "    all_metrics = {}\n",
    "    pr_curves = {}\n",
    "    roc_curves = {}\n",
    "    \n",
    "    # Train and evaluate each model\n",
    "    for name, clf in classifiers.items():\n",
    "        print(f\"\\n{'='*50}\\nTraining {name}...\\n{'='*50}\")\n",
    "        \n",
    "        if name == \"ChemCoBERT\":\n",
    "            # Deep learning model setup\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "            train_dataset = DrugInteractionDataset(\n",
    "                X_train_smiles[:, 0], X_train_smiles[:, 1], y_train, tokenizer\n",
    "            )\n",
    "            test_dataset = DrugInteractionDataset(\n",
    "                X_test_smiles[:, 0], X_test_smiles[:, 1], y_test, tokenizer\n",
    "            )\n",
    "            \n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            model = CoAttentionModel().to(device)\n",
    "            optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # Train\n",
    "            model, _ = train_model(\n",
    "                model, \n",
    "                DataLoader(train_dataset, batch_size=8, shuffle=True),\n",
    "                DataLoader(test_dataset, batch_size=8),\n",
    "                optimizer, criterion, device, epochs=10\n",
    "            )\n",
    "            \n",
    "            # Load best model\n",
    "            model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "            \n",
    "            # Evaluate if test labels are available\n",
    "            if y_test is not None:\n",
    "                _, test_metrics = evaluate_model(\n",
    "                    model, \n",
    "                    DataLoader(test_dataset, batch_size=8),\n",
    "                    criterion, device\n",
    "                )\n",
    "                all_metrics[name] = test_metrics\n",
    "                \n",
    "                # Get predictions for curves\n",
    "                test_probs, _ = predict(model, DataLoader(test_dataset, batch_size=128), device)\n",
    "                \n",
    "                # Store curves\n",
    "                precision, recall, _ = precision_recall_curve(y_test, test_probs)\n",
    "                pr_auc = auc(recall, precision)\n",
    "                pr_curves[name] = {\n",
    "                    \"precision\": precision,\n",
    "                    \"recall\": recall,\n",
    "                    \"auc\": pr_auc\n",
    "                }\n",
    "                \n",
    "                fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
    "                roc_auc = auc(fpr, tpr)\n",
    "                roc_curves[name] = {\n",
    "                    \"fpr\": fpr,\n",
    "                    \"tpr\": tpr,\n",
    "                    \"auc\": roc_auc\n",
    "                }\n",
    "                \n",
    "                # 保存预测分数和原始标签\n",
    "                save_prediction_scores(\n",
    "                    name, \n",
    "                    X_test_smiles, \n",
    "                    test_probs, \n",
    "                    y_test, \n",
    "                    cutoff=test_metrics['Cutoff']\n",
    "                )\n",
    "            \n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), f\"{name.replace(' ', '_')}_model.pth\")\n",
    "\n",
    "        else:\n",
    "            # Traditional ML models\n",
    "\n",
    "            clf.fit(X_train_num, y_train)\n",
    "    \n",
    "            # Save model\n",
    "            import joblib\n",
    "            joblib.dump(clf, f\"{name.replace(' ', '_')}_model.joblib\")\n",
    "    \n",
    "            if y_test is not None:\n",
    "                # Predict probabilities\n",
    "                if hasattr(clf, \"predict_proba\"):\n",
    "                    test_probs = clf.predict_proba(X_test_num)[:, 1]\n",
    "                else:\n",
    "                    test_probs = clf.decision_function(X_test_num)\n",
    "                    test_probs = (test_probs - test_probs.min()) / (test_probs.max() - test_probs.min())\n",
    "                \n",
    "                # 使用最佳阈值计算指标\n",
    "                cutoff, TN, FP, FN, TP = best_confusion_matrix(y_test, test_probs)\n",
    "                \n",
    "                all_metrics[name] = {\n",
    "                    \"AUC\": roc_auc_score(y_test, test_probs),\n",
    "                    \"Cutoff\": cutoff,\n",
    "                    \"Sensitivity\": TP / (TP + FN),\n",
    "                    \"Specificity\": TN / (TN + FP),\n",
    "                    \"Kappa\": cohen_kappa_score(y_test, (test_probs >= cutoff).astype(int)),\n",
    "                    \"MCC\": matthews_corrcoef(y_test, (test_probs >= cutoff).astype(int)),\n",
    "                    \"F1\": f1_score(y_test, (test_probs >= cutoff).astype(int)),\n",
    "                }\n",
    "                \n",
    "                # 保存预测分数和原始标签\n",
    "                save_prediction_scores(\n",
    "                    name, \n",
    "                    X_test_smiles, \n",
    "                    test_probs, \n",
    "                    y_test, \n",
    "                    cutoff=cutoff\n",
    "                )\n",
    "        \n",
    "            # Store curves\n",
    "            precision, recall, _ = precision_recall_curve(y_test, test_probs)\n",
    "            pr_auc = auc(recall, precision)\n",
    "            pr_curves[name] = {\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"auc\": pr_auc\n",
    "            }\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(y_test, test_probs)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            roc_curves[name] = {\n",
    "                \"fpr\": fpr,\n",
    "                \"tpr\": tpr,\n",
    "                \"auc\": roc_auc\n",
    "            }\n",
    "\n",
    "    # Print final metrics if test labels are available\n",
    "    if y_test is not None:\n",
    "        print(\"\\nFinal Test Metrics:\")\n",
    "        for name, metrics in all_metrics.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"{metric}: {value:.4f}\")\n",
    "        \n",
    "        # Plot curves\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # PR Curve\n",
    "        plt.subplot(122)\n",
    "        for name in classifiers:\n",
    "            if name in pr_curves:\n",
    "                # Set ChemCoBERT to red, others to default colors\n",
    "                if name == \"ChemCoBERT\":\n",
    "                    plt.plot(pr_curves[name][\"recall\"], pr_curves[name][\"precision\"], \n",
    "                            '#E41A1C', linewidth=3, \n",
    "                            label=f\"{name} (AUC={pr_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"Decision Tree\":\n",
    "                    plt.plot(pr_curves[name][\"recall\"], pr_curves[name][\"precision\"], \n",
    "                            '#A65628', linewidth=2.5, linestyle='--',  # 虚线\n",
    "                            label=f\"{name} (AUC={pr_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"AdaBoost\":\n",
    "                    plt.plot(pr_curves[name][\"recall\"], pr_curves[name][\"precision\"], \n",
    "                            '#4DAF4A', linewidth=2.5, \n",
    "                            label=f\"{name} (AUC={pr_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"GBDT\":\n",
    "                    plt.plot(pr_curves[name][\"recall\"], pr_curves[name][\"precision\"], \n",
    "                            '#FF7F00', linewidth=2.5, \n",
    "                            label=f\"{name} (AUC={pr_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"K-NN\":\n",
    "                    plt.plot(pr_curves[name][\"recall\"], pr_curves[name][\"precision\"], \n",
    "                            '#984EA3', linewidth=2.5, \n",
    "                            label=f\"{name} (AUC={pr_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"Naive Bayes\":\n",
    "                    plt.plot(pr_curves[name][\"recall\"], pr_curves[name][\"precision\"], \n",
    "                            '#377EB8', linewidth=2.5,  alpha=0.4,  # 半透明\n",
    "                            label=f\"{name} (AUC={pr_curves[name]['auc']:.4f})\")\n",
    "        \n",
    "        plt.xlabel(\"Recall\", fontweight='bold')\n",
    "        plt.ylabel(\"Precision\", fontweight='bold')\n",
    "        plt.title(\"Precision-Recall Curve\", fontweight='bold')\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ROC Curve\n",
    "        plt.subplot(121)\n",
    "        for name in classifiers:\n",
    "            if name in roc_curves:\n",
    "                # Set ChemCoBERT to red, others to default colors\n",
    "                if name == \"ChemCoBERT\":\n",
    "                    plt.plot(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"], \n",
    "                            '#E41A1C', linewidth=3, \n",
    "                            label=f\"{name} (AUC={roc_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"Decision Tree\":\n",
    "                    plt.plot(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"], \n",
    "                            '#A65628', linewidth=2.5, linestyle='--',  # 虚线\n",
    "                            label=f\"{name} (AUC={roc_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"AdaBoost\":\n",
    "                    plt.plot(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"], \n",
    "                            '#4DAF4A', linewidth=2.5, \n",
    "                            label=f\"{name} (AUC={roc_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"GBDT\":\n",
    "                    plt.plot(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"], \n",
    "                            '#FF7F00', linewidth=2.5, \n",
    "                            label=f\"{name} (AUC={roc_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"K-NN\":\n",
    "                    plt.plot(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"], \n",
    "                            '#984EA3', linewidth=2.5, \n",
    "                            label=f\"{name} (AUC={roc_curves[name]['auc']:.4f})\")\n",
    "                elif name == \"Naive Bayes\":\n",
    "                    plt.plot(roc_curves[name][\"fpr\"], roc_curves[name][\"tpr\"], \n",
    "                            '#377EB8', linewidth=2.5,  alpha=0.4,  # 半透明\n",
    "                            label=f\"{name} (AUC={roc_curves[name]['auc']:.4f})\")\n",
    "        \n",
    "        \n",
    "        plt.xlabel(\"False Positive Rate\", fontweight='bold')\n",
    "        plt.ylabel(\"True Positive Rate\", fontweight='bold')\n",
    "        plt.title(\"Receiver Operating Characteristic Curve\", fontweight='bold')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "        # 修改预测部分（在main函数中）\n",
    "        if y_test is None:\n",
    "            print(\"\\nGenerating predictions for test set...\")\n",
    "            predictions = {}\n",
    "            cutoffs = {}  # 存储各模型的最佳阈值\n",
    "            \n",
    "            for name in classifiers:\n",
    "                if name == \"ChemCoBERT\":\n",
    "                    # Load model and tokenizer\n",
    "                    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-77M-MLM\")\n",
    "                    test_dataset = DrugInteractionDataset(\n",
    "                        X_test_smiles[:, 0], X_test_smiles[:, 1], None, tokenizer\n",
    "                    )\n",
    "                    \n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    model = CoAttentionModel().to(device)\n",
    "                    model.load_state_dict(torch.load(f\"{name.replace(' ', '_')}_model.pth\"))\n",
    "                    \n",
    "                    # Predict\n",
    "                    test_probs, _ = predict(model, DataLoader(test_dataset, batch_size=128), device)\n",
    "                    predictions[name] = test_probs\n",
    "                    # 使用训练数据确定最佳阈值\n",
    "                    train_probs, _ = predict(model, DataLoader(train_dataset, batch_size=128), device)\n",
    "                    fpr, tpr, thresholds = roc_curve(y_train, train_probs)\n",
    "                    cutoffs[name] = find_optimal_cutoff(tpr, fpr, thresholds)\n",
    "                    \n",
    "                    # 保存预测分数\n",
    "                    save_prediction_scores(\n",
    "                        name, \n",
    "                        X_test_smiles, \n",
    "                        test_probs, \n",
    "                        None, \n",
    "                        cutoff=cutoffs[name]\n",
    "                    )\n",
    "                else:\n",
    "                    # Load traditional model\n",
    "                    import joblib\n",
    "                    clf = joblib.load(f\"{name.replace(' ', '_')}_model.joblib\")\n",
    "                    \n",
    "                    # Predict probabilities\n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        test_probs = clf.predict_proba(X_test_num)[:, 1]\n",
    "                    else:\n",
    "                        test_probs = clf.decision_function(X_test_num)\n",
    "                        test_probs = (test_probs - test_probs.min()) / (test_probs.max() - test_probs.min())\n",
    "                    \n",
    "                    predictions[name] = test_probs\n",
    "                    # 使用训练数据确定最佳阈值\n",
    "                    if hasattr(clf, \"predict_proba\"):\n",
    "                        train_probs = clf.predict_proba(X_train_num)[:, 1]\n",
    "                    else:\n",
    "                        train_probs = clf.decision_function(X_train_num)\n",
    "                        train_probs = (train_probs - train_probs.min()) / (train_probs.max() - train_probs.min())\n",
    "                    fpr, tpr, thresholds = roc_curve(y_train, train_probs)\n",
    "                    cutoffs[name] = find_optimal_cutoff(tpr, fpr, thresholds)\n",
    "                    \n",
    "                    # 保存预测分数\n",
    "                    save_prediction_scores(\n",
    "                        name, \n",
    "                        X_test_smiles, \n",
    "                        test_probs, \n",
    "                        None, \n",
    "                        cutoff=cutoffs[name]\n",
    "                    )\n",
    "            \n",
    "            # 保存预测结果和阈值\n",
    "            pred_df = pd.DataFrame({\n",
    "                **predictions,\n",
    "                **{f\"{name}_cutoff\": [cutoffs[name]]*len(X_test_smiles) for name in classifiers}\n",
    "            })\n",
    "            pred_df.to_csv(\"test_predictions_with_cutoffs.csv\", index=False)\n",
    "            print(\"✅ Predictions saved to test_predictions_with_cutoffs.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8016501,
     "sourceId": 12684971,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
